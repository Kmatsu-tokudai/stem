{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un6JI184NrMb"
      },
      "outputs": [],
      "source": [
        "# 学習済みのベクトルデータの準備\n",
        "import os\n",
        "import gdown\n",
        "if not os.path.exists('/content/data'):\n",
        "    os.mkdir('/content/data')\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1BwRe4FnPi26qzpUZ1r98x9PVkWLaA1d6'\n",
        "gdown.download(url, './data/stem_vec.tar.gz', quiet=False)\n",
        "!tar zxvf /content/data/stem_vec.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 類義語を検索してみる\n",
        "import gensim\n",
        "import warnings\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "if not os.path.exists('/content/stem_vec/word2vec.gensim.egotw.model'):\n",
        "    raise ValueError(\"ERROR: 学習済みword2vecモデルが存在しません。\")\n",
        "\n",
        "# 学習済みWord2vecを読込\n",
        "wv = Word2Vec.load(\"/content/stem_vec/word2vec.gensim.egotw.model\")\n",
        "\n",
        "# 検索\n",
        "input_word = '学生'\n",
        "if not input_word in wv:\n",
        "    print(\"Input word [%s] is not included in the model\" % input_word)\n",
        "else:\n",
        "    results = wv.most_similar(positive=[input_word])\n",
        "    for result in results:\n",
        "        print(result)"
      ],
      "metadata": {
        "id": "Ol8PPnYmSuyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word2vecの学習を追加で行う\n",
        "\n",
        "# 形態素解析器MeCabのラッパーライブラリfugashiをインストール\n",
        "!pip install fugashi[unidic-lite]\n",
        "from fugashi import Tagger\n",
        "\n",
        "txt = '私は徳島大学の学生です。'\n",
        "\n",
        "tagger = Tagger('-Owakati')\n",
        "\n",
        "# 形態素解析する関数\n",
        "def wakatigaki(txt):\n",
        "    words = []\n",
        "    wakati = tagger.parse(txt)\n",
        "    for w in wakati.split(' '):\n",
        "        words.append(w)\n",
        "    return words\n",
        "\n",
        "for w in wakatigaki(txt):\n",
        "    print(w)"
      ],
      "metadata": {
        "id": "E2wMtcuCTmiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# コーパスの準備\n",
        "# テキストデータ(livedoor news corpus)をMeCab(fugashi)で分かち書き処理\n",
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "!tar zxvf ldcc-20140209.tar.gz\n",
        "# テキストを1つのファイルにまとめる(すべてのファイルで3行目以降が本文)\n",
        "import glob\n",
        "wf = open('./all.txt', 'w')\n",
        "for dir in glob.glob('/content/text/*'):\n",
        "    for path in glob.glob(f'{dir}/*.txt'):\n",
        "        l = 0\n",
        "        for line in open(path):\n",
        "            line = line.rstrip()\n",
        "            if line == '':\n",
        "                continue\n",
        "\n",
        "            l += 1\n",
        "            if l >= 3:\n",
        "                wf.write(' '.join(wakatigaki(line)) + '\\n')\n",
        "wf.close\n"
      ],
      "metadata": {
        "id": "eleV1u1RUt3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# livedoor news corpusの分かち書きテキストを\n",
        "# 追加で学習させる\n",
        "from gensim.models import word2vec\n",
        "corpus = '/content/all.txt'\n",
        "newmodel = '/content/stem_vec/w2v.new.livedoor.model'\n",
        "sentences = word2vec.Text8Corpus(corpus)\n",
        "\n",
        "model = word2vec.Word2Vec.load(\"/content/stem_vec/word2vec.gensim.egotw.model\")\n",
        "model.build_vocab(sentences, update=True)\n",
        "model.train(sentences, total_examples=model.corpus_count, epochs=10)\n",
        "model.save(newmodel)"
      ],
      "metadata": {
        "id": "95Sv6PCWWsI7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "20e57d6a-fc01-4d39-b52a-99a1f833ce99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wrimeコーパスをダウンロード\n",
        "!git clone https://github.com/ids-cv/wrime"
      ],
      "metadata": {
        "id": "dhgwu0IErGNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 文書ベクトルを作成する\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# 単語分散表現を格納したリストを作成する\n",
        "def make_vec(fn, model):\n",
        "    vlist = []\n",
        "    for word in fn:\n",
        "        if not word in model:\n",
        "            vlist.append(np.array([0.0]*50))\n",
        "        else:\n",
        "            vlist.append(model[word])\n",
        "    vlist = np.asarray(vlist)\n",
        "    return vlist\n",
        "\n",
        "# コーパス中の各文の感情ラベルを取得する\n",
        "def getEmotionLabels( df ):\n",
        "    cols = ['Writer_Joy', 'Writer_Sadness',\n",
        "            'Writer_Anticipation','Writer_Surprise',\n",
        "            'Writer_Anger', 'Writer_Fear', 'Writer_Disgust', 'Writer_Trust']\n",
        "    emotions = ['喜び', '悲しみ', '期待', '驚き', '怒り', '怖れ', '嫌悪', '信頼']\n",
        "    emos = []\n",
        "    for ev in df[cols].values:\n",
        "        emos.append( emotions[np.argmax(ev)] )\n",
        "    return emos\n",
        "\n",
        "# 類似度の閾値（threshold）を設定して，\n",
        "# 閾値以上のものだけを辞書に格納する\n",
        "def filtering_result(sims, threshold):\n",
        "    sh = {}\n",
        "    for i, s in enumerate(sims):\n",
        "        if s >= threshold:\n",
        "            sh[i] = s\n",
        "    return sh\n",
        "\n",
        "\n",
        "# メイン関数\n",
        "def main():\n",
        "\n",
        "    # 作成済みのlivedoorの分散表現のモデルを読み込んでおく\n",
        "    livedoor_model_path = '/content/stem_vec/w2v.new.livedoor.model'\n",
        "    livedoor_model = word2vec.Word2Vec.load(livedoor_model_path)\n",
        "\n",
        "    # 検索対象となるテキストコーパスを読み込む\n",
        "    df = pd.read_csv('./wrime/wrime-ver1.tsv', sep='\\t')\n",
        "    df = df[df['Train/Dev/Test'] == 'test']\n",
        "    sents = df['Sentence'].values\n",
        "    emos = getEmotionLabels(df)\n",
        "\n",
        "    print(len(sents))\n",
        "\n",
        "    cps = []\n",
        "    with open('./wrime.txt', 'w') as wf:\n",
        "        for s in sents:\n",
        "            wl = wakatigaki(s)\n",
        "            sen = ' '.join(wl)\n",
        "            wf.write(f'{sen}\\n')\n",
        "            cps.append(sen)\n",
        "\n",
        "    # コーパスからTF-IDFを計算\n",
        "    print(len(cps))\n",
        "    vect = TfidfVectorizer(analyzer=lambda x: x.split(\" \"), dtype=np.float32, token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
        "    wv = vect.fit_transform(cps)\n",
        "    fn = vect.get_feature_names()\n",
        "\n",
        "    vlist = make_vec(fn, livedoor_model)\n",
        "\n",
        "    vsm = wv.sum(axis=1)\n",
        "    docvec = (wv @ vlist) / vsm\n",
        "\n",
        "    waka = wakatigaki('今日は全身がだるくて，何もやる気が起きません！')\n",
        "    avgv = np.array([0.0]*50)\n",
        "    count = 0.0\n",
        "    for w in waka:\n",
        "        if w in model:\n",
        "            if np.sum(avgv) == 0:\n",
        "                avgv = livedoor_model[w].copy()\n",
        "            else:\n",
        "                avgv += livedoor_model[w].copy()\n",
        "            count += 1.0\n",
        "\n",
        "    # 入力されたクエリと検索対象文書集合間の類似度を計算\n",
        "    query = np.array([avgv])\n",
        "    query = np.nanmean(query, axis=0).reshape(1, -1)\n",
        "    sims = cosine_sim(docvec, query)\n",
        "    # 類似度が0.5以上の結果だけを取得する\n",
        "    sh = filtering_result(sims, 0.5)\n",
        "\n",
        "    sh = {}\n",
        "    for i, s in enumerate(sims):\n",
        "        if s >= 0.5:\n",
        "            sh[i] = s\n",
        "\n",
        "    topk = 30 # 上位30件のみ表示\n",
        "    emok = {'喜び':0, '悲しみ':0, '期待':0, '驚き':0, '怒り':0, '怖れ':0, '嫌悪':0, '信頼':0}\n",
        "    for k,v in sorted(sh.items(), key=lambda x:x[1], reverse=True)[:topk]:\n",
        "        print(\"SID:[%d], Sim:[%.3lf] <%s> ==> [%s]\" % (k,v, cps[k], emos[k]) )\n",
        "        emok[emos[k]] += 1\n",
        "\n",
        "    print(emok)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyMoE_WPNznH",
        "outputId": "6705fdac-414b-4c16-cf91-b8c2b33782f0"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n",
            "2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:77: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:79: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SID:[1048], Sim:[0.890] <今日 は 子ども ら が 早く 寝 た の で 勉強 しよ と 思っ た が 最近 の 無理 の 反動 で 全く 何 も やる気 が 起き ない> ==> [怖れ]\n",
            "SID:[1169], Sim:[0.838] <久し ぶり に いい 感じ の 眠気 が 。 今日 は 寝よう か な 。 嫌 な こと は 忘れる 。 嫌 な こと 特に ない けど 。> ==> [喜び]\n",
            "SID:[1976], Sim:[0.836] <いつ も 通り に 筋トレ は し てる けど 、 九州 大雨 が 気 に なっ て 仕方 が ない 。 早く 雨 が やん で くれる 事 を 祈る しか ない 。 無力 さ を 痛感 し ます 。 皆 さん どう か ご 無事 で い て ください !> ==> [悲しみ]\n",
            "SID:[346], Sim:[0.831] <もう 秋 だ って の に 、 暑く て 汗 かい てる の 私 だけ ？> ==> [悲しみ]\n",
            "SID:[510], Sim:[0.830] <いっつ も こう やっ て 何 かしら ぬけ落ち て て 悲しく なる 。 (°_°)> ==> [悲しみ]\n",
            "SID:[1602], Sim:[0.826] <一 度 子供 達 と 寝 落ち し て しまう と 、 その 後 起き て も ずっと 頭痛 が ・ ・ ・ 。 何 で だろう ？ ？ ？ 謎 。> ==> [怖れ]\n",
            "SID:[408], Sim:[0.820] <と いう か 、 今日 めっちゃ 暑く ない ？> ==> [驚き]\n",
            "SID:[699], Sim:[0.818] <で も ぜーんぶ 終わっ た 一 週間 と ちょっと 後 は 東京 ！ その 時 に は ストレス フリー に なっ てる はず ！ かぜ なおす ！> ==> [喜び]\n",
            "SID:[1916], Sim:[0.818] <トレーニング を 始め て から 身体 が 変わる 。 最近 職場 で 「 どう やっ たら 痩せれ た ん ？ 」 と 聞か れる こと が 増え た 。 いや 、 聞か なく て も ホント は みんな わかっ てる 。 やる しか ない 事 は すでに 頭 で わかっ て いる 。 取り組み 始める か どう か 、 それ を 続ける か どう か が 問題 。> ==> [喜び]\n",
            "SID:[1017], Sim:[0.817] <超 元気 な 子ども ら の 相手 と 気圧 の 関係 で かなり 頭痛 が する の に 、 ズーム 会議 だっ ！ つっ て ガンガン 電話 かけ て くる の で 、 せめて も の 反抗 で 電話 の 最中 洗濯 物 畳ん で て やっ た よく 考え て も 考え なくっ て も 仕事 が 嫌い だ> ==> [怒り]\n",
            "SID:[1444], Sim:[0.816] <悩ん で クヨクヨ し てる 時 。 「 一 ヶ月 後 に この 出来事 を 覚え て いる か ？ 」 と 考え ます 。 大体 は 一 か月 後 に は 忘れ てる 出来事 な ん だ から 、 今 悩ん で も 仕方ない と 思う よう に し て い ます 。> ==> [期待]\n",
            "SID:[617], Sim:[0.811] <なに に も やる気 が 起き ない ? で も やる こと いっぱい ある ?> ==> [嫌悪]\n",
            "SID:[1717], Sim:[0.809] <それ は そう と ウォーキング デッド の 季節 が 今年 も もう すぐ やっ て 来 ます ね 。> ==> [喜び]\n",
            "SID:[987], Sim:[0.807] <熱中 し たい なー 。 いま 熱中 し てる こと は 何 だろう か 。> ==> [喜び]\n",
            "SID:[1113], Sim:[0.807] <何 も か も うまく いか なかっ た 一日 、 と 妻 が 言っ て いる が 、 果たして … 笑> ==> [喜び]\n",
            "SID:[502], Sim:[0.806] <２ 日 病人 と とも に ゴロゴロ し て た から 、 スイッチ 入れ て やる こと やら なきゃ だ けど 、 1 人 で も ゴロゴロ し て しまう 。> ==> [喜び]\n",
            "SID:[1423], Sim:[0.805] <天気 が 良い から 出掛け たい と 思っ て て も 、 なかなか 準備 が 出来 ない ー 。> ==> [悲しみ]\n",
            "SID:[583], Sim:[0.800] <卵 雑炊 が 食べ たい 。 明日 は この 髪色 で 大丈夫 か 心配 。 明日 で 村 の 人 に 会う の 最後 か な ? と か 思っ たら 寂しい な ? 。> ==> [怖れ]\n",
            "SID:[1714], Sim:[0.797] <29 度 で 涼しい って 思う って やばく ない ？ 前 は これ で 暑い って 思っ て た の に 。> ==> [驚き]\n",
            "SID:[1655], Sim:[0.797] <明日 の ハロウィン 。 何 か し たい けど 今年 は あまり やら ない か なー ら 、> ==> [悲しみ]\n",
            "SID:[1983], Sim:[0.797] <ジム 終わり その まま 子供 の チア ダンス 練習 付添 へ （ 汗 ） コロナ で 親 は 外 で 待た なあ かん の で ウォーキング し たら 汗だく でし た 少し は 休憩 せ なあ き ませ ん ね> ==> [怖れ]\n",
            "SID:[1856], Sim:[0.796] <そろそろ 寝 … られる か は 分から ない けど お やすみ チャレンジ し ます> ==> [悲しみ]\n",
            "SID:[1937], Sim:[0.793] <先ほど 友人 に 「 停滞 期 つら すぎる 」 と 相談 さ れ まし た 。 辛い よ な 。 頑張っ てる の に 成果 が 見え ない と 。 で も それ を 乗り越え た 先 に しか 理想 の 体 や 夢 に 手 が 届か ない の も 事実 。 落ち込む 事 も ある けど 、 やっ て やろう や> ==> [信頼]\n",
            "SID:[303], Sim:[0.793] <ショック の あまり 、 体調 崩し て 午前 で 早退 し た 。 みんな 元気 か な …> ==> [悲しみ]\n",
            "SID:[221], Sim:[0.792] <もう 夕方 だ けど 何 食べよっ か な …> ==> [喜び]\n",
            "SID:[1898], Sim:[0.792] <最近 鼻 の 蛇口 が ぶっ壊れ て 返信 考える どころ で は ない の ほんと 何> ==> [怒り]\n",
            "SID:[1548], Sim:[0.791] <最近 また 寝る の が 遅く なっ て き て 寝 不足 気味 です 。 夜更かし は 百害 あっ て 一利 なし な の に 。> ==> [悲しみ]\n",
            "SID:[597], Sim:[0.791] <一 時間 待っ て も こ ない ！ ！ ガス 会社 な ん な の ！ ！ 急い で 帰っ て き た の に ? 時間 守ら ない ひと きらい ！ ！ ！ \\ n なんか 食べ物 で も 持っ て き たら 許そう 。> ==> [怒り]\n",
            "SID:[1054], Sim:[0.790] <今日 は 暑い くらい の 日 です と いう けど 、 朝 気温 が 低い 限り そんな 気 に も なれ ず どう し て も 厚着 を し て しまう> ==> [悲しみ]\n",
            "SID:[1313], Sim:[0.789] <体調 管理 も 業務 の 責任 範囲 って 言わ れ て も さ 、 \\ n \\ n こっち も 健康 状態 の 時 と 変わり なく 普通 に 生活 し て たら 急 に 喉 痛く なっ て 熱 出 た ん です よ 。 好き で ウイルス 採集 し てる 訳 じゃ ない ん です よ 。 \\ n \\ n そもそも 極寒 の 外 で の 仕事 で 風邪 引い た ん で むしろ 責任 取っ て ください と 言い たい 。> ==> [喜び]\n",
            "{'喜び': 9, '悲しみ': 9, '期待': 1, '驚き': 2, '怒り': 3, '怖れ': 4, '嫌悪': 1, '信頼': 1}\n"
          ]
        }
      ]
    }
  ]
}