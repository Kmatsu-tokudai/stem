{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un6JI184NrMb"
      },
      "outputs": [],
      "source": [
        "# 学習済みのベクトルデータの準備\n",
        "import os\n",
        "import gdown\n",
        "if not os.path.exists('/content/data'):\n",
        "    os.mkdir('/content/data')\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1BwRe4FnPi26qzpUZ1r98x9PVkWLaA1d6'\n",
        "gdown.download(url, './data/stem_vec.tar.gz', quiet=False)\n",
        "!tar zxvf /content/data/stem_vec.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 類義語を検索してみる\n",
        "import gensim\n",
        "import warnings\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "if not os.path.exists('/content/stem_vec/word2vec.gensim.egotw.model'):\n",
        "    raise ValueError(\"ERROR: 学習済みword2vecモデルが存在しません。\")\n",
        "\n",
        "# 学習済みWord2vecを読込\n",
        "wv = Word2Vec.load(\"/content/stem_vec/word2vec.gensim.egotw.model\")\n",
        "\n",
        "# 検索\n",
        "input_word = '学生'\n",
        "if not input_word in wv:\n",
        "    print(\"Input word [%s] is not included in the model\" % input_word)\n",
        "else:\n",
        "    results = wv.most_similar(positive=[input_word])\n",
        "    for result in results:\n",
        "        print(result)"
      ],
      "metadata": {
        "id": "Ol8PPnYmSuyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word2vecの学習を追加で行う\n",
        "\n",
        "# 形態素解析器MeCabのラッパーライブラリfugashiをインストール\n",
        "!pip install fugashi[unidic-lite]\n",
        "from fugashi import Tagger\n",
        "\n",
        "txt = '私は徳島大学の学生です。'\n",
        "\n",
        "tagger = Tagger('-Owakati')\n",
        "\n",
        "# 形態素解析する関数\n",
        "def wakatigaki(txt):\n",
        "    words = []\n",
        "    wakati = tagger.parse(txt)\n",
        "    for w in wakati.split(' '):\n",
        "        words.append(w)\n",
        "    return words\n",
        "\n",
        "for w in wakatigaki(txt):\n",
        "    print(w)"
      ],
      "metadata": {
        "id": "E2wMtcuCTmiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# コーパスの準備\n",
        "# テキストデータ(livedoor news corpus)をMeCab(fugashi)で分かち書き処理\n",
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "!tar zxvf ldcc-20140209.tar.gz\n",
        "# テキストを1つのファイルにまとめる(すべてのファイルで3行目以降が本文)\n",
        "import glob\n",
        "wf = open('./all.txt', 'w')\n",
        "for dir in glob.glob('/content/text/*'):\n",
        "    for path in glob.glob(f'{dir}/*.txt'):\n",
        "        l = 0\n",
        "        for line in open(path):\n",
        "            line = line.rstrip()\n",
        "            if line == '':\n",
        "                continue\n",
        "\n",
        "            l += 1\n",
        "            if l >= 3:\n",
        "                wf.write(' '.join(wakatigaki(line)) + '\\n')\n",
        "wf.close\n"
      ],
      "metadata": {
        "id": "eleV1u1RUt3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# livedoor news corpusの分かち書きテキストを\n",
        "# 追加で学習させる\n",
        "from gensim.models import word2vec\n",
        "corpus = '/content/all.txt'\n",
        "newmodel = '/content/stem_vec/w2v.new.livedoor.model'\n",
        "sentences = word2vec.Text8Corpus(corpus)\n",
        "\n",
        "model = word2vec.Word2Vec.load(\"/content/stem_vec/word2vec.gensim.egotw.model\")\n",
        "model.build_vocab(sentences, update=True)\n",
        "model.train(sentences, total_examples=model.corpus_count, epochs=10)\n",
        "model.save(newmodel)"
      ],
      "metadata": {
        "id": "95Sv6PCWWsI7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "20e57d6a-fc01-4d39-b52a-99a1f833ce99"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wrimeコーパスをダウンロード\n",
        "!git clone https://github.com/ids-cv/wrime"
      ],
      "metadata": {
        "id": "dhgwu0IErGNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 文書ベクトルを作成する\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# コサイン類似度の計算\n",
        "def cosine_sim(X, Y):\n",
        "    return (X @ Y.T) / np.sqrt(np.nansum(np.power(X, 2), axis=1) * np.nansum(np.power(Y, 2), axis=1))\n",
        "\n",
        "# 単語分散表現を格納したリストを作成する\n",
        "def make_vec(fn, model):\n",
        "    vlist = []\n",
        "    for word in fn:\n",
        "        if not word in model:\n",
        "            vlist.append(np.array([0.0]*50))\n",
        "        else:\n",
        "            vlist.append(model[word])\n",
        "    vlist = np.asarray(vlist)\n",
        "    return vlist\n",
        "\n",
        "# コーパス中の各文の感情ラベルを取得する\n",
        "def getEmotionLabels( df ):\n",
        "    cols = ['Writer_Joy', 'Writer_Sadness',\n",
        "            'Writer_Anticipation','Writer_Surprise',\n",
        "            'Writer_Anger', 'Writer_Fear', 'Writer_Disgust', 'Writer_Trust']\n",
        "    emotions = ['喜び', '悲しみ', '期待', '驚き', '怒り', '怖れ', '嫌悪', '信頼']\n",
        "    emos = []\n",
        "    for ev in df[cols].values:\n",
        "        emos.append( emotions[np.argmax(ev)] )\n",
        "    return emos\n",
        "\n",
        "# 類似度の閾値（threshold）を設定して，\n",
        "# 閾値以上のものだけを辞書に格納する\n",
        "def filtering_result(sims, threshold):\n",
        "    sh = {}\n",
        "    for i, s in enumerate(sims):\n",
        "        if s >= threshold:\n",
        "            sh[i] = s\n",
        "    return sh\n",
        "\n",
        "\n",
        "# メイン関数\n",
        "def main():\n",
        "\n",
        "    # 作成済みのlivedoorの分散表現のモデルを読み込んでおく\n",
        "    livedoor_model_path = '/content/stem_vec/w2v.new.livedoor.model'\n",
        "    livedoor_model = word2vec.Word2Vec.load(livedoor_model_path)\n",
        "\n",
        "    # 検索対象となるテキストコーパスを読み込む\n",
        "    df = pd.read_csv('./wrime/wrime-ver1.tsv', sep='\\t')\n",
        "    df = df[df['Train/Dev/Test'] == 'test']\n",
        "    sents = df['Sentence'].values\n",
        "    emos = getEmotionLabels(df)\n",
        "\n",
        "    print(len(sents))\n",
        "\n",
        "    cps = []\n",
        "    with open('./wrime.txt', 'w') as wf:\n",
        "        for s in sents:\n",
        "            wl = wakatigaki(s)\n",
        "            sen = ' '.join(wl)\n",
        "            wf.write(f'{sen}\\n')\n",
        "            cps.append(sen)\n",
        "\n",
        "    # コーパスからTF-IDFを計算\n",
        "    print(len(cps))\n",
        "    vect = TfidfVectorizer(analyzer=lambda x: x.split(\" \"), dtype=np.float32, token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
        "    wv = vect.fit_transform(cps)\n",
        "    fn = vect.get_feature_names()\n",
        "\n",
        "    vlist = make_vec(fn, livedoor_model)\n",
        "\n",
        "    vsm = wv.sum(axis=1)\n",
        "    docvec = (wv @ vlist) / vsm\n",
        "\n",
        "    waka = wakatigaki('今日は全身がだるくて，何もやる気が起きません！')\n",
        "    avgv = np.array([0.0]*50)\n",
        "    count = 0.0\n",
        "    for w in waka:\n",
        "        if w in model:\n",
        "            if np.sum(avgv) == 0:\n",
        "                avgv = livedoor_model[w].copy()\n",
        "            else:\n",
        "                avgv += livedoor_model[w].copy()\n",
        "            count += 1.0\n",
        "\n",
        "    # 入力されたクエリと検索対象文書集合間の類似度を計算\n",
        "    query = np.array([avgv])\n",
        "    query = np.nanmean(query, axis=0).reshape(1, -1)\n",
        "    sims = cosine_sim(docvec, query)\n",
        "    # 類似度が0.5以上の結果だけを取得する\n",
        "    sh = filtering_result(sims, 0.5)\n",
        "\n",
        "    topk = 30 # 上位30件のみ表示\n",
        "    emok = {'喜び':0, '悲しみ':0, '期待':0, '驚き':0, '怒り':0, '怖れ':0, '嫌悪':0, '信頼':0}\n",
        "    for k,v in sorted(sh.items(), key=lambda x:x[1], reverse=True)[:topk]:\n",
        "        print(\"SID:[%d], Sim:[%.3lf] <%s> ==> [%s]\" % (k,v, cps[k], emos[k]) )\n",
        "        emok[emos[k]] += 1\n",
        "\n",
        "    print(emok)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "YyMoE_WPNznH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}